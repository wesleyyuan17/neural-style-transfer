{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from skimage import io, transform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision.models import vgg19 \n",
    "from torchsummary import summary\n",
    "\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image = 'degas.jpg'\n",
    "content_image = 'climbing1.jpg'\n",
    "n_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(output_image, target_style_representation, target_content_representation, feature_maps, model, optimizer, n_steps, a, b):\n",
    "    for iter in range(n_steps):\n",
    "        # get feature maps of current output image\n",
    "        _ = model(output_image)\n",
    "        output_content_representation = get_content_representation(feature_maps.copy())\n",
    "        output_style_representation = get_style_representation(feature_maps.copy())\n",
    "\n",
    "        # calculate errors and backprop\n",
    "        loss = a * content_loss(target_content_representation, output_content_representation) + b * style_loss(target_style_representation, output_style_representation)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(style_image, content_image, n_steps, a=0.1, b=1e-3):\n",
    "    # for saving model later\n",
    "    style_name = style_image.split('.')[0]\n",
    "    content_name = content_image.split('.')[0]\n",
    "\n",
    "    # load images and models\n",
    "    style_image = read_image('../style_images/{}'.format(style_image)).transpose([2, 0, 1])\n",
    "    content_image = torch.Tensor(read_image('../content_images/{}'.format(content_image)).transpose([2, 0, 1]))\n",
    "    style_image = torch.Tensor(transform.resize(style_image, content_image.shape))\n",
    "    content_image = content_image / content_image.max()\n",
    "\n",
    "    # get white noise image to run gradient descent on for output image\n",
    "    # output_image = Variable(torch.rand(content_image.shape), requires_grad=True)\n",
    "    output_image = Variable(content_image.detach().clone(), requires_grad=True)\n",
    "\n",
    "    model = vgg19(pretrained=True).features # only want feature maps, don't need classifier portion\n",
    "    model.eval()\n",
    "\n",
    "    # freeze parameters so transfer learning gradient descent just affects images, doesn't retrain network\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # summary(model, INPUT_SIZE)\n",
    "    # print(model)\n",
    "\n",
    "    # get style/content representations which are static for a given model/image\n",
    "    feature_maps = set_hooks(model)\n",
    "    _ = model(content_image)\n",
    "    target_content_representation = get_content_representation({k: v.detach() for k, v in feature_maps.items()})\n",
    "    _ = model(style_image)\n",
    "    target_style_representation = get_style_representation({k: v.detach() for k, v in feature_maps.items()})\n",
    "\n",
    "    optimizer = optim.SGD([output_image], lr=1e-2, momentum=0.9)\n",
    "    output_image = train(output_image, \n",
    "                         target_style_representation, \n",
    "                         target_content_representation, \n",
    "                         feature_maps, \n",
    "                         model, \n",
    "                         optimizer, \n",
    "                         n_steps, \n",
    "                         a, b)\n",
    "\n",
    "    output_image = output_image.detach().numpy()\n",
    "    output_image = np.transpose(output_image, [1, 2, 0])\n",
    "    # print(output_image.min(), output_image.max())\n",
    "    output_image = (output_image - output_image.min()) / (output_image.max() - output_image.min())\n",
    "    plt.imsave('{}_{}.png'.format(content_name, style_name), output_image)\n",
    "    plt.imshow(output_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(style_image, content_image, n_steps)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1cc167b126b20d7f4a938aeb9e4dec04d917a87950e162caa71079bb49a81dcd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
